{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-16T20:32:47.647923Z",
     "start_time": "2025-05-16T20:32:45.197312Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directories\n",
    "train_dir = 'chest-xray/train'\n",
    "test_dir = 'chest-xray/test'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:32:48.772925Z",
     "start_time": "2025-05-16T20:32:48.739591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)"
   ],
   "id": "4f4ac78ff72a054d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:32:51.742152Z",
     "start_time": "2025-05-16T20:32:51.739530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# K-Fold setup\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "batch_size = 32"
   ],
   "id": "f331a1d0fe8e3def",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T20:32:53.388860Z",
     "start_time": "2025-05-16T20:32:53.386292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create the model\n",
    "def create_model():\n",
    "    model = models.densenet121(pretrained=True)\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, 2)\n",
    "    return model.to(device)\n",
    "\n",
    "# Store best model state dict and fold metrics\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "history_all_folds = []"
   ],
   "id": "96a98538ad41df2a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-16T20:32:55.512061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# K-Fold Cross Validation\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
    "    print(f\"\\n Fold {fold + 1}/{k_folds}\")\n",
    "    train_sub = Subset(full_dataset, train_ids)\n",
    "    val_sub = Subset(full_dataset, val_ids)\n",
    "    train_loader = DataLoader(train_sub, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = create_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_total += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss = val_loss_total / len(val_loader)\n",
    "        val_acc = accuracy_score(val_labels, val_preds) * 100\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/10 | Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save best model state\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    history_all_folds.append(history)"
   ],
   "id": "a299ca023dc2523f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayuri/PycharmProjects/Lab0/pythonProject/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/mayuri/PycharmProjects/Lab0/pythonProject/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.1357, Acc: 95.09% | Val Loss: 0.0789, Acc: 97.13%\n",
      "Epoch 2/10 | Train Loss: 0.0801, Acc: 97.10% | Val Loss: 0.0601, Acc: 97.61%\n",
      "Epoch 3/10 | Train Loss: 0.0611, Acc: 97.77% | Val Loss: 0.0525, Acc: 97.99%\n",
      "Epoch 4/10 | Train Loss: 0.0559, Acc: 97.82% | Val Loss: 0.0350, Acc: 98.56%\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#  Plot Loss and Accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "for fold, hist in enumerate(history_all_folds):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist['train_loss'], label=f'Train Fold {fold+1}')\n",
    "    plt.plot(hist['val_loss'], linestyle='--', label=f'Val Fold {fold+1}')\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist['train_acc'], label=f'Train Fold {fold+1}')\n",
    "    plt.plot(hist['val_acc'], linestyle='--', label=f'Val Fold {fold+1}')\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a8d07d0f89731274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation on Test Set using Best Model\n",
    "print(\"\\n Evaluating Best Model on Test Set...\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "model = create_model()\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())"
   ],
   "id": "8b40952cc74650c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"\\n Test Accuracy: {acc * 100:.2f}%\")\n",
    "print(f\" Test F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['NORMAL', 'PNEUMONIA']))"
   ],
   "id": "c3cefafd7ef7f3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['NORMAL', 'PNEUMONIA'], yticklabels=['NORMAL', 'PNEUMONIA'])\n",
    "plt.title(\"Confusion Matrix on Test Set\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5812b68ecda8341b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
